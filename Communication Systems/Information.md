Information concers change and can be quantitified

Entropy is a measure of information.

## Information Sources
An information source must be random (to some extrent) to convey information

A digitial source will:
- Generate symbols from a finite alphabet at regular intervals
	Symbols are [[Mutually Exclusive Events]]
- Sucessive symbols are often [[Independant Events]]
- Size of the alphabet is called the [[Radix]] of the source

## Information Theory
Information is the resolution of uncertainty
- Uncertainty is measured using [[Probability]]
- Hence, information conveyed by a symbol $A$ is denoted by $$I(A)= f(P(A))$$Where:
- $P(A)$ denotes the [[a priori]] probability of $A$, and $f(P(A))$ denotes some appropriate function (to be ddetermined).

## Information content
Consider an [[Equiprobable]]